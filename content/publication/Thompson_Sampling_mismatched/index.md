---
abstract: "We consider Thompson Sampling (TS) for linear combinatorial
  semi-bandits and subgaussian rewards. We propose the first known TS whose
  finite-time regret does not scale exponentially with the dimension of the
  problem. We further show the “mismatched sampling paradox”: A learner who
  knows the rewards distributions and samples from the correct posterior
  distribution can perform exponentially worse than a learner who does not know
  the rewards and simply samples from a well-chosen Gaussian posterior."
slides: ""
url_pdf: https://arxiv.org/abs/2410.05441
publication_types:
  - Comference Paper
authors:
  - me
  - Richard Combes
author_notes: []
publication: In *Neural Information Processing Systems 2024*
summary: We propose the first known Thompson Sampling for combinatorial bandits
  whose finite-time regret does not scale exponentially with the dimension of
  the problem. Suprisingly, considering any subgaussian distribution as a
  gaussian can produce exponentialy better result.
url_dataset: ""
url_project: ""
publication_short: In *NeurIPS2024* (Spotlight)
url_source: ""
url_video: ""
title: "Thompson Sampling For Combinatorial Bandits: Polynomial Regret and
  Mismatched Sampling Paradox"
doi: ""
featured: false
tags: []
projects: []
image:
  caption: ""
  focal_point: ""
  preview_only: false
  filename: featured.jpg
date: 2024-12-09T00:00:00.000Z
url_slides: ""
publishDate: 2024-12-01T00:00:00.000Z
url_poster: ""
url_code: https://github.com/RaymZhang/CTS-Mismatched-Paradox
---


<!-- {{% callout note %}}
Click the *Cite* button above to demo the feature to enable visitors to import publication metadata into their reference management software.
{{% /callout %}}

{{% callout note %}}
Create your slides in Markdown - click the *Slides* button to check out the example.
{{% /callout %}}

Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). -->
